{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83028631-54f1-4fc8-92cd-3db34086e6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0c516ab-0c8f-4bd1-b4d2-16b3a2609993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tracking and Evaluation Experiments with MLflow\n",
    "\n",
    "In this demo, we will learn how to use MLflow integration in Databricks to track and evaluate experiments.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "* Create MLflow experiment to track development\n",
    "\n",
    "* Log runs to capture individual changes\n",
    "\n",
    "* Calculate evaluation metrics using mlflow.evaluate API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d0116ca-a6ca-4bad-bcf5-3b1247995f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Demo Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "419d3a5e-c95c-4413-a260-8556f5817ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install dependencies and configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d6e6d64-cdc0-4454-b712-b8a92228db12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow databricks_genai_inference==0.2.3 evaluate torch transformers textstat\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12125dd-ff3f-4e1b-81de-53fa93d7baf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reset MLflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c19d845-01d7-489c-9737-95db6efd811e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Default MLflow experiment name in Databricks is /Users/{username}/{notebook filename}\n",
    "notebook_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "experiment = mlflow.get_experiment_by_name(notebook_name)\n",
    "print(experiment)\n",
    "\n",
    "from mlflow.utils.databricks_utils import get_databricks_host_creds\n",
    "from mlflow.utils.request_utils import augmented_raise_for_status\n",
    "from mlflow.utils.rest_utils import http_request\n",
    "import time\n",
    "\n",
    "def delete_all_runs(experiment_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Bulk delete all runs in an experiment.\n",
    "    \n",
    "    :param experiment_id: The ID of the experiment containing the runs to delete.\n",
    "    :return: The number of runs deleted.\n",
    "    \"\"\"\n",
    "    # Current time in milliseconds\n",
    "    max_timestamp_millis = int(time.time() * 1000)\n",
    "    \n",
    "    json_body = {\n",
    "        \"experiment_id\": experiment_id, \n",
    "        \"max_timestamp_millis\": max_timestamp_millis,\n",
    "        \"max_runs\": 10000  # Maximum allowed value\n",
    "    }\n",
    "    \n",
    "    response = http_request(\n",
    "        host_creds=get_databricks_host_creds(),\n",
    "        endpoint=\"/api/2.0/mlflow/databricks/runs/delete-runs\",\n",
    "        method=\"POST\",\n",
    "        json=json_body,\n",
    "    )\n",
    "    \n",
    "    augmented_raise_for_status(response)\n",
    "    return response.json()[\"runs_deleted\"]\n",
    "\n",
    "experiment_id = experiment.experiment_id\n",
    "deleted_runs_count = delete_all_runs(experiment_id)\n",
    "print(f\"Deleted {deleted_runs_count} runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f25318e-5079-4585-ba4a-227d3fe91a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create an experiment to compare multiple models\n",
    "Here we will use MLflow to run an experiment to compare multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e15c0c73-9c77-4e6c-986b-820e1a4405ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Evaluation Dataset\n",
    "In order to do that we are going to first define an evaluation data set to use as input for each model to compare the outputs against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e726d4e3-2ae2-4e6f-a343-abd851caa5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e412e84-638a-41f1-aaeb-8acd2de4bd90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Experiment Configuration and Variables\n",
    "Next, we are going to define the models we want to compare and their configurations.  Keeping the model parameters the same here allows us to focus on the differences between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a09bbeff-4e87-4007-963c-b2e58484771e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "          \"databricks-dbrx-instruct\", \n",
    "          \"databricks-llama-2-70b-chat\", \n",
    "          \"databricks-mixtral-8x7b-instruct\"\n",
    "        ]\n",
    "max_tokens=128\n",
    "temperature=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f72e212-bda3-4eee-92ff-98975a1064fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create runs using MLFlow\n",
    "Now we are going to use MLFlow to automate the experiment by having it generate runs and evaluations for each of the models against our eval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8f0ff0-2e77-4d6a-b64c-e69fbe8664b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.deployments\n",
    "import pandas as pd\n",
    "import mlflow.metrics.genai\n",
    "\n",
    "mlflow.deployments.set_deployments_target(\"databricks\")\n",
    "\n",
    "# Loop throw our models\n",
    "for model in MODELS:\n",
    "  # Create a MLflow run for each\n",
    "  with mlflow.start_run() as run:\n",
    "    mlflow.log_params({\n",
    "      \"model\": model,\n",
    "      \"max_tokens\": max_tokens,\n",
    "      \"temperature\": temperature,\n",
    "    })\n",
    "\n",
    "    # Calculating evaluation metrics for each run\n",
    "    results = mlflow.evaluate(\n",
    "      model=f\"endpoints:/{model}\",\n",
    "      data=eval_data,\n",
    "      targets=\"ground_truth\",\n",
    "      inference_params={\n",
    "        \"max_tokens\":   max_tokens,\n",
    "        \"temperature\":  temperature,\n",
    "      },\n",
    "      model_type=\"question-answering\",\n",
    "      extra_metrics=[mlflow.metrics.token_count(), mlflow.metrics.latency()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c04220cf-b720-496f-b9dd-c22de06a341e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Go over MLflow tracking artifacts for a run\n",
    "* Select the **beaker** icon on the right navigation bar\n",
    "  * Select a run to open up details in MLflow run UI\n",
    "* Go over\n",
    "  * **Overview** tab\n",
    "  * **System Metric** tab\n",
    "  * **Artifacts** tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70368de6-5b9a-443c-a714-bd8b70a71b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compare models using the MLflow\n",
    "Now that we have ran our experiment against the three models and calculated the metrics we can compare it using MLflow.\n",
    "\n",
    "* Select the **beaker** icon on the right navigation bar\n",
    "  * Select **Experiment UI** to open up the MLflow experiments UI\n",
    "* Select the **Chart** icon to compare the runs in graphical view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8da35a2-235f-4ef6-9793-8aff1b6eac7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "In this demo we learned how MLflow integration into Databricks make it easy to track and log our data science and machine learning experiments.  As well as tools to store and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c358616-2dc3-4f5d-a124-aa2f4fe111f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "GenAI-GS-01B Mlflow Experiments and Evaluation Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
